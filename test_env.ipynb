{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75c0ffd0-86b6-4dd8-a830-0e9555f89f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from itertools import count\n",
    "import torch\n",
    "import torchvision.transforms as transforms \n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "transform = transforms.Compose([ \n",
    "    transforms.ToTensor() \n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fffc2ff-48c6-418d-bad7-c7c2f3d55269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38a19e10-5a94-4de2-85e4-346d3516f97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started in position:  (28948, 27057)\n"
     ]
    }
   ],
   "source": [
    "import gym_envs\n",
    "\n",
    "import gymnasium as gym\n",
    "from gym.wrappers import TransformReward, TransformObservation\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "env = gym.make(\n",
    "    \"gym_envs/WSIWorldEnv-v1\",\n",
    "    render_mode=\"human\",\n",
    "    patch_size=(256, 256),\n",
    "    resize_thumbnail=False,\n",
    ")\n",
    "# env = TransformReward(env, lambda r: torch.tensor([r]))\n",
    "# env = TransformObservation(env, lambda obs: (transform(obs[0]).unsqueeze(0), transform(obs[1]).unsqueeze(0)))\n",
    "\n",
    "patch_size = env.unwrapped.wsi_wrapper.patch_size\n",
    "thumbnail_size = env.unwrapped.wsi_wrapper.thumbnail_size\n",
    "num_actions = env.action_space.n  # Number of actions in your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "927919e0-42a3-4822-8916-a10f0021c103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(828, 1650)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thumbnail_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb2996c9-cb2b-46de-aa1c-03184736f386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started in position:  (46490, 14718)\n",
      "Already in the max magnification, can't zoom-in more. Returning\n",
      "Already in the max magnification, can't zoom-in more. Returning\n",
      "Already in the max magnification, can't zoom-in more. Returning\n",
      "Already in the max magnification, can't zoom-in more. Returning\n",
      "Already in the max magnification, can't zoom-in more. Returning\n",
      "Already in the max magnification, can't zoom-in more. Returning\n",
      "Already in the max magnification, can't zoom-in more. Returning\n",
      "Already in the min magnification, can't zoom-out more. Returning\n",
      "Already in the min magnification, can't zoom-out more. Returning\n",
      "Already in the max magnification, can't zoom-in more. Returning\n",
      "Already in the max magnification, can't zoom-in more. Returning\n",
      "Already in the max magnification, can't zoom-in more. Returning\n",
      "Already in the max magnification, can't zoom-in more. Returning\n",
      "Already in the max magnification, can't zoom-in more. Returning\n",
      "Already in the max magnification, can't zoom-in more. Returning\n",
      "Already in the max magnification, can't zoom-in more. Returning\n",
      "Already in the max magnification, can't zoom-in more. Returning\n",
      "Already in the max magnification, can't zoom-in more. Returning\n",
      "Already in the min magnification, can't zoom-out more. Returning\n",
      "Already in the max magnification, can't zoom-in more. Returning\n",
      "Already in the max magnification, can't zoom-in more. Returning\n",
      "Already in the max magnification, can't zoom-in more. Returning\n",
      "Already in the max magnification, can't zoom-in more. Returning\n",
      "Already in the max magnification, can't zoom-in more. Returning\n",
      "Already in the max magnification, can't zoom-in more. Returning\n",
      "Already in the max magnification, can't zoom-in more. Returning\n",
      "Already in the max magnification, can't zoom-in more. Returning\n",
      "Already in the max magnification, can't zoom-in more. Returning\n",
      "Already in the max magnification, can't zoom-in more. Returning\n",
      "Already in the max magnification, can't zoom-in more. Returning\n"
     ]
    }
   ],
   "source": [
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dce8d53-66dd-4b53-9ad6-5eccc1faad62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started in position:  (49341, 14710)\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "print(type(observation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc3d73b6-086c-4a82-bc37-4197ac61f152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64)\n",
      "(512, 512)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "204481334"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, patch_size, thumbnail_size, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        print(patch_size)\n",
    "        print(thumbnail_size)\n",
    "        self.patch_width, self.patch_height = patch_size\n",
    "        self.thumbnail_width, self.thumbnail_height = thumbnail_size\n",
    "\n",
    "        # Define the CNN layers for the current view\n",
    "        self.current_view_conv = nn.Conv2d(\n",
    "            in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.current_view_fc = nn.Linear(\n",
    "            6 * self.patch_width * self.patch_height, 128\n",
    "        )  # Update 256*256 based on the actual size of the feature maps\n",
    "\n",
    "        # Define the CNN layers for the bird eye view\n",
    "        self.birdeye_view_conv = nn.Conv2d(\n",
    "            in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.birdeye_view_fc = nn.Linear(\n",
    "            6 * self.thumbnail_width * self.thumbnail_height, 128\n",
    "        )  # Update based on the actual size\n",
    "\n",
    "        # # Attention mechanism\n",
    "        # self.attention = nn.Linear(256, 256)\n",
    "\n",
    "        # Fully connected layers after attention\n",
    "        self.fc1 = nn.Linear(256, 32)\n",
    "        self.fc2 = nn.Linear(32, num_actions)\n",
    "\n",
    "    def forward(self, current_view, birdeye_view):\n",
    "        # Forward pass for current view\n",
    "        x_current = F.relu(self.current_view_conv(current_view))\n",
    "        x_current = x_current.view(\n",
    "            -1, 6 * self.patch_width * self.patch_height\n",
    "        )  # Update based on the actual size\n",
    "        x_current = F.relu(self.current_view_fc(x_current))\n",
    "\n",
    "        # Forward pass for bird eye view\n",
    "        x_birdeye = F.relu(self.birdeye_view_conv(birdeye_view))\n",
    "        x_birdeye = x_birdeye.view(\n",
    "            -1, 6 * self.thumbnail_width * self.thumbnail_height\n",
    "        )  # Update based on the actual size\n",
    "        x_birdeye = F.relu(self.birdeye_view_fc(x_birdeye))\n",
    "\n",
    "        # Concatenate the outputs of both branches\n",
    "        x_combined = torch.cat((x_current, x_birdeye), dim=1)\n",
    "\n",
    "        # # Attention mechanism\n",
    "        # attention_weights = F.softmax(self.attention(x_combined), dim=1)\n",
    "        # x_attention = torch.sum(attention_weights * x_combined, dim=0)\n",
    "\n",
    "        # Fully connected layers after attention\n",
    "        x = F.relu(self.fc1(x_combined))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "policy_net = DQN(patch_size, thumbnail_size, num_actions)\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2558ad4b-eb64-44ac-8445-5bb7d4bbb485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1614, -0.1038,  0.0124, -0.1598,  0.0903,  0.1584],\n",
       "        [-0.2258, -0.1049,  0.0045, -0.1760,  0.1008,  0.1760],\n",
       "        [-0.1746, -0.0962,  0.0404, -0.1262,  0.1116,  0.1982],\n",
       "        [-0.1914, -0.0359,  0.0191, -0.1886,  0.1034,  0.1082],\n",
       "        [-0.1650, -0.0867, -0.0478, -0.0974,  0.0506,  0.1893],\n",
       "        [-0.1873, -0.0750, -0.0223, -0.1449,  0.0799,  0.1914],\n",
       "        [-0.2075, -0.1189,  0.0196, -0.1643,  0.1060,  0.1634],\n",
       "        [-0.1609, -0.0469, -0.0017, -0.1644,  0.1105,  0.1666],\n",
       "        [-0.1275, -0.1187,  0.0103, -0.1452,  0.1015,  0.1705],\n",
       "        [-0.1661, -0.1272, -0.0112, -0.1349,  0.0810,  0.1658]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net(torch.randn(10, 3, 64, 64,), torch.randn(10, 3, thumbnail_size[0], thumbnail_size[1],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d10a08ce-a9ef-417b-9d29-a29e4a957826",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, strategy, num_actions, device):\n",
    "        self.current_step = 0\n",
    "        self.strategy = strategy\n",
    "        self.num_actions = num_actions\n",
    "        self.device = device\n",
    "\n",
    "    def select_action(self, observation, policy_net):\n",
    "        rate = strategy.get_exploration_rate(self.current_step)\n",
    "        self.current_step += 1\n",
    "\n",
    "        if rate > random.random():\n",
    "            action = random.randrange(self.num_actions)\n",
    "            return torch.tensor([action]).to(self.device)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                return policy_net(observation[0], observation[1]).argmax().reshape(-1).to(self.device) # 7DI HNAYA .reshape(-1)\n",
    "\n",
    "class QValues():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_current(policy_net, patches, bird_views, actions):\n",
    "        return policy_net(patches, bird_views).gather(dim=1, index=actions.unsqueeze(-1)) # gather to get the q value for the selected action that was judged as the best back then\n",
    "\n",
    "    @staticmethod\n",
    "    def get_next(target_net, next_patches, next_bird_views):\n",
    "        values = target_net(next_patches, next_bird_views).max(dim=1)[0].detach()\n",
    "        return values\n",
    "\n",
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "\n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "        self.push_count += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size\n",
    "\n",
    "class EpsilonGreedyStrategy():\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "\n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return self.end + (self.start - self.end) * \\\n",
    "               math.exp(-1. * current_step * self.decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fc5e176-c67c-410a-969a-50a510de7b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "# dqn = DQN(em.action_space.n).to(device)\n",
    "\n",
    "Experience = namedtuple(\n",
    "    'Experience',\n",
    "    ('patches', 'bird_views', 'action', 'next_patches', 'next_bird_views', 'reward')\n",
    ")\n",
    "\n",
    "def extract_tensors(experiences):\n",
    "    batch = Experience(*zip(*experiences))\n",
    "    \n",
    "    patches = torch.cat(batch.patches) # torch.Size([B, 3, 256, 256])\n",
    "    bird_views = torch.cat(batch.bird_views) # torch.Size([B, 3, 828, 1650])\n",
    "\n",
    "    actions = torch.cat(batch.action)\n",
    "    rewards = torch.cat(batch.reward)\n",
    "\n",
    "    next_patches = torch.cat(batch.next_patches) # torch.Size([144, 256, 256])\n",
    "    next_bird_views = torch.cat(batch.next_bird_views) # torch.Size([144, 828, 1650])\n",
    "    \n",
    "    return (patches, bird_views, actions, rewards, next_patches, next_bird_views)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "635ba23d-76d6-4e63-abe7-1b19c3c34dc0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m batch \u001b[38;5;241m=\u001b[39m Experience(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mexperiences))\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got tuple"
     ]
    }
   ],
   "source": [
    "batch = Experience(*zip(*experiences))\n",
    "torch.cat(batch.observation, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "476eedd1-ce9b-45a0-9bb8-e01f22733eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 48\n",
    "gamma = 0.99\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.001\n",
    "target_update = 10\n",
    "memory_size = 17000\n",
    "lr = 0.001\n",
    "num_episodes = 100000\n",
    "\n",
    "em = env\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "agent = Agent(strategy, em.action_space.n, device)\n",
    "memory = ReplayMemory(memory_size)\n",
    "\n",
    "policy_net = DQN(patch_size, thumbnail_size, em.action_space.n).to(device)\n",
    "target_net = DQN(patch_size, thumbnail_size, em.action_space.n).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)\n",
    "\n",
    "episode_durations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f051bd4b-ccd6-44c8-a6db-adf6acf1bc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started in position:  (43570, 25821)\n",
      "Already in the max magnification, can't zoom-in more. Returning\n",
      "Already in the min magnification, can't zoom-out more. Returning\n",
      "Already in the min magnification, can't zoom-out more. Returning\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m patches, bird_views, actions, rewards, next_patches, next_bird_views \u001b[38;5;241m=\u001b[39m extract_tensors(experiences)\n\u001b[1;32m     17\u001b[0m current_q_values \u001b[38;5;241m=\u001b[39m QValues\u001b[38;5;241m.\u001b[39mget_current(policy_net, patches, bird_views, actions)\n\u001b[0;32m---> 18\u001b[0m next_q_values \u001b[38;5;241m=\u001b[39m \u001b[43mQValues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_patches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_bird_views\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m target_q_values \u001b[38;5;241m=\u001b[39m (next_q_values \u001b[38;5;241m*\u001b[39m gamma) \u001b[38;5;241m+\u001b[39m rewards\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(current_q_values, target_q_values\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n",
      "Cell \u001b[0;32mIn[9], line 28\u001b[0m, in \u001b[0;36mQValues.get_next\u001b[0;34m(target_net, next_patches, next_bird_views)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_next\u001b[39m(target_net, next_patches, next_bird_views):\n\u001b[0;32m---> 28\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_patches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_bird_views\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "File \u001b[0;32m~/micromamba/envs/rl/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/rl/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 45\u001b[0m, in \u001b[0;36mDQN.forward\u001b[0;34m(self, current_view, birdeye_view)\u001b[0m\n\u001b[1;32m     41\u001b[0m x_birdeye \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbirdeye_view_conv(birdeye_view))\n\u001b[1;32m     42\u001b[0m x_birdeye \u001b[38;5;241m=\u001b[39m x_birdeye\u001b[38;5;241m.\u001b[39mview(\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m6\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthumbnail_width \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthumbnail_height\n\u001b[1;32m     44\u001b[0m )  \u001b[38;5;66;03m# Update based on the actual size\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m x_birdeye \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbirdeye_view_fc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_birdeye\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Concatenate the outputs of both branches\u001b[39;00m\n\u001b[1;32m     48\u001b[0m x_combined \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x_current, x_birdeye), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/rl/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/rl/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/rl/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for episode in range(num_episodes):\n",
    "    observation, info = em.reset()\n",
    "\n",
    "    for timestep in count():\n",
    "        action = agent.select_action(observation, policy_net)\n",
    "        next_observation, reward, terminated, truncated, info = env.step(action.item())\n",
    "        \n",
    "        patches, bird_views = observation\n",
    "        next_patches, next_bird_views = next_observation\n",
    "        \n",
    "        memory.push(Experience(patches, bird_views, action, next_patches, next_bird_views, reward))\n",
    "        observation = next_observation\n",
    "\n",
    "        if memory.can_provide_sample(batch_size):\n",
    "            experiences = memory.sample(batch_size)\n",
    "            patches, bird_views, actions, rewards, next_patches, next_bird_views = extract_tensors(experiences)\n",
    "            current_q_values = QValues.get_current(policy_net, patches, bird_views, actions)\n",
    "            next_q_values = QValues.get_next(target_net, next_patches, next_bird_views)\n",
    "            target_q_values = (next_q_values * gamma) + rewards\n",
    "\n",
    "            loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if terminated:\n",
    "            episode_durations.append(timestep)\n",
    "            break\n",
    "\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "em.close()\n",
    "em_val.close()\n",
    "\n",
    "MODEL_PATH = ''\n",
    "torch.save(target_net.state_dict(), MODEL_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
